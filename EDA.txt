EXPLORATORY DATA ANALYSIS

EDA is a set of statistical approaches to explore and understand data beyond hypothesis testing and experimentation. It unearth any underlying insights.

It is always better to explore each data set using multiple exploratory techniques and compare the results. The goal of this step is to understand the dataset, identify the missing values & outliers if any using visual and quantitative methods to get a sense of the story it tells. It suggests the next logical steps, questions or areas of research for your project.

Cleaning Data:

1. Fix rows and column data:
- Delete Incorrect rows if any
- Delete summary rows
- Specify column names, if missing.
- Delete unnecessary column with junk data.
- Split column into multiple columns, like for Date – split into year, date, month columns
- Merge columns into single column – ex: Merge First name, Last name into Full Name column
2. Fix missing values:
- Remove missing data rows,  if they don’t mean much value.
- Replace the missing data with ‘mean’ value
- Replace the missing data with ‘median’ value
- df.isnull().sum(axis=0) –> calculate missing data column wise.
3. Standardizing data:
- Bring up all data to the same scale. (Normalization)
- Look out for outliers.
- Making all character data, either in lower case or upper case.
- Date in string format to Date format.
4. Fixing invalid values
5. Filter data (like removing duplicates, etc)
-------------------------------------------------------------------------------------------
IDENTIFICATION OF VARIALBLES AND DATA TYPES:

Commands to use:
1. .shape()             -> returns the shape of an array (dimensions)
2. .info()              -> prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage
3. .describe()          -> generates descriptive statistics
4. ._get_numeric_data() -> returns a subset of the DataFrame’s columns based on the column dtypes (returns columns with numeric values)
5. cat = list(set(df.columns) - set(df._get_numeric_data().columns)) -> categorical columns
-------------------------------------------------------------------------------------------
NON-GRAPHICAL UNIVARIATE ANALYSIS:

1. .value_counts() -> count of all the unique values in a column
2. .nunique()      -> number of distinct observations in a column
3. .unique()       -> list of unique values in the dataset
4. Using conditions on the data
-------------------------------------------------------------------------------------------
GRAPHICAL UNIVARIATE ANALYSIS:

1. Histogram
2. Box Plots
3. Scatter plot
4. Violin plot
-------------------------------------------------------------------------------------------
OUTLIER DETECTION:

“Observation which deviates so much from other observations as to arouse suspicion it was generated by a different mechanism” — Hawkins(1980)

Outliers are extreme values that deviate from other observations in the data, which might indicate a variability in a measurement, experimental errors or a novelty. In other words, an outlier is an observation that diverges from an overall pattern on a sample.

Detecting outliers is of major importance for almost any quantitative discipline (ie: Physics, Economy, Finance, Machine Learning, Cyber Security). In machine learning and in any quantitative discipline the quality of data is as important as the quality of a prediction or classification model.

In the process of producing, collecting, processing and analyzing data, outliers can come from many sources and hide in many dimensions. Those that are not a product of an error are called novelties.

Types of outliers:
1. Univariate   -> found when looking at a distribution of values in a single feature space
2. Multivariate -> found in a n-dimensional space (of n-features)

Flavors of outliers depending on the environment:
1. Point      -> single data points that lay far from the rest of the distribution
2. Contextual -> noise in data, such as punctuation symbols when realizing text analysis or background noise signal when doing speech recognition
3. Collective -> subsets of novelties in data such as a signal that may indicate the discovery of new phenomena

Most common causes of outliers on a data set:
1. Data entry errors (human errors)
2. Measurement errors (instrument errors)
3. Experimental errors (data extraction or experiment planning/executing errors)
4. Intentional (dummy outliers made to test detection methods)
5. Data processing errors (data manipulation or data set unintended mutations)
6. Sampling errors (extracting or mixing data from wrong or various sources)
7. Natural (not an error, novelties in data)

When trying to detect outliers in a dataset it is very important to keep in mind the context and try to answer the question: “Why do I want to detect outliers?” The meaning of your findings will be dictated by the context.

Also, when starting an outlier detection quest you have to answer two important questions about your dataset:
1. Which and how many features am I taking into account to detect outliers? (univariate / multivariate)
2. Can I assume a distribution(s) of values for my selected features? (parametric / non-parametric)

Some of the most popular methods for outlier detection are:
1. Z-Score or Extreme Value Analysis (parametric)
2. Probabilistic and Statistical Modeling (parametric)
3. Linear Regression Models (PCA, LMS)
4. Proximity Based Models (non-parametric)
5. Information Theory Models
6. High Dimensional Outlier Detection Methods (high dimensional sparse data)

1. Z-Score (Parametric method)
The z-score or standard score of an observation is a metric that indicates how many standard deviations a data point is from the sample’s mean, assuming a gaussian distribution. 

Very frequently data points are not described by a gaussian distribution, this problem can be solved by applying transformations to data ie: scaling it. After making the appropriate transformations to the selected feature space of the dataset, the z-score of any data point can be calculated with the following expression:
                                z = x-u/sigma
                                u - Mean
                                sigma - standard deviation
When computing the z-score for each sample in the data set a threshold must be specified. Some good ‘thumb-rule’ thresholds can be: 2.5, 3, 3.5 or more standard deviations. By ‘tagging’ or removing the data points that lay beyond a given threshold we are classifying data into outliers and not outliers

--code---
def remove_outliers(df):
    outliers = {}
    for col in df.columns:
        if str(df[col].dtype) != 'object':
            df = df[np.abs(df[col]-df[col].mean()) < (3*df[col].std())]
            orls = df[~(np.abs(df[col]-df[col].mean()) < (3*df[col].std()))]
            outliers = pd.Dataframe(orls)
    return df, outliers
---------

2. Percentile based
--code---
# col -> columsn for which interquartile method is used
Q1 = df['col'].quantile(0.25)
Q3 = df['col'].quantile(0.75)
IQR = Q3 - Q1    #IQR is interquartile range. 

# create filter using IQR values to remove outliers that are below 5% above 95% of 'col'
filter = (df['col'] >= Q1 - 1.5 * IQR) & (df['col']]<= Q3 + 1.5 *IQR)
filter_flow = df['col'].loc[filter]  
---------

3. Dbscan (Non-parametric method)
4. Isolation Forests (Non-parametric method)
-------------------------------------------------------------------------------------------
CORRELATION ANALYSIS

--code---
_ = sns.heatmap(df.corr(), annot=True, vmin= -1 , vmax=1) -> rectangular data as a color-encoded matrix
-------------------------------------------------------------------------------------------
MISSING VALUE TREATMENT

Types of missing values:
1. Missing values that Pandas can detect
- .isnull() -> a value in the column is null or not (recognizes missing value and “NA”)
2. Missing values that have different formats -> e.g. ["n/a", "na", "--"]
- manually correct the missing values
--code---
# Making a list of missing value types
missing_values = ["n/a", "na", "--"]
df = pd.read_csv("property data.csv", na_values = missing_values)
# this code converts other missing value formats to the one recognized by pandas
---------
3. Unexpected missing values
e.g. if a feature is expected to be a string, but there’s a numeric type, then technically this is also a missing value.

- df.isnull().sum()        -> Total missing values for each feature
- df.isnull().values.any() -> any missing values in the dataframe
- df.isnull().sum()        -> no of missing values per column
- df.isnull().sum().sum()  -> no of missing values in total in a dataframe

Treating missing values:
1. Delete those rows
2. Replace them
- Replace missing values with a number -> df['col'].fillna(125, inplace=True)
- Location based replacement -> df.loc[2,'col'] = 125
- very common way to replace missing values is using a median